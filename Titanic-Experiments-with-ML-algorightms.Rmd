---
title: "Titanic: Experiments with ML Algoriths"
author: "Sanjeev Gadre"
date: "August 15, 2019"
output:
    md_document:
        toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r libraries, echo=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ROCR)
library(e1071)
library(neuralnet)
library(randomForest)
library(gbm)

```

```{r functions}
na.count = function (dat){
  dat %>% apply(., 2, is.na) %>% apply(.,2,sum) %>% .[.!=0]
}

```

##  Exploratory Data Analysis and Pre-processing

### Getting the Data

1.  We start by getting a sense of the train set and its structure.

```{r get-data-1}
train = read.csv("./data/train.csv")

head(train)
str(train)

```

1.  We add a column "Survived" to the test set and assign a default value of "0".
2.  Finally, we combide train and test sets, for easier changes to the dataframe structure, imputing missing values and analysis.

```{r get-data-2}

test = read.csv("./data/test.csv")
test$Survived = 0

dat = rbind(train, test)
indx = 1:nrow(train)

```

### Data Pre-processing

We run the following data pre-processing steps:
1. Convert the feature type for `Survived`, `Pclass` from `<int>` to `<fctr>`.
2. Convert the feature type for `Name`, `Cabin` from `<fctr>` to `<char>`.
3. Convert the feature type for `Ticket` from `<fct>` to `<int>`.

Finally, we get separate the train and test data sets.

```{r pre-proc-1}

dat$Survived = factor(dat$Survived)
dat$Pclass = factor(dat$Pclass)
dat$Name = as.character(dat$Name)
dat$Cabin = as.character(dat$Cabin)
dat$Ticket = as.integer(dat$Ticket)
str(dat)

train = dat[indx,]
test = dat[-indx,]
rm(dat)

```

### Exploratory Data Analysis

1.  From the list of features available, it is reasonable to guess that the survival of a passenger would likely **not** depend on `Name`, `Ticket` and `Cabin`. 
2.  For the remaining features we get a distribution of their respective values for the train and test set to ascertain any significant differences between the two.

```{r eda-1}
plot.age = ggplot()+ 
    geom_density(data = train, aes(x=Age, fill = "Train"), size = 1, na.rm = TRUE, alpha = 0.5)+ 
    geom_density(data = test, aes(x=Age, fill = "Test"), size = 1, na.rm = TRUE, alpha = 0.5)+
    scale_fill_manual(name = "Subsets", values = c(Train = "red", Test = "blue"))+ ylab("")
plot.sibsp = ggplot()+ 
    geom_density(data = train, aes(x=SibSp, fill = "Train"), size = 1, na.rm = TRUE, alpha = 0.5)+ 
    geom_density(data = test, aes(x=SibSp, fill = "Test"), size = 1, na.rm = TRUE, alpha = 0.5)+
    scale_fill_manual(name = "Subsets", values = c(Train = "red", Test = "blue"))+ ylab("")
plot.parch = ggplot()+ 
    geom_density(data = train, aes(x=Parch, fill = "Train"), size = 1, na.rm = TRUE, alpha = 0.5)+ 
    geom_density(data = test, aes(x=Parch, fill = "Test"), size = 1, na.rm = TRUE, alpha = 0.5)+
    scale_fill_manual(name = "Subsets", values = c(Train = "red", Test = "blue"))+ ylab("")
plot.fare = ggplot()+ 
    geom_density(data = train, aes(x=Fare, fill = "Train"), size = 1, na.rm = TRUE, alpha = 0.5)+ 
    geom_density(data = test, aes(x=Fare, fill = "Test"), size = 1, na.rm = TRUE, alpha = 0.5)+
    scale_fill_manual(name = "Subsets", values = c(Train = "red", Test = "blue"))+ ylab("")
grid.arrange(plot.age, plot.sibsp, plot.parch, plot.fare, ncol = 2)

train.subset = table(train$Pclass) %>% prop.table() %>% round(4) %>% as.vector()
test.subset = table(test$Pclass) %>% prop.table() %>% round(4) %>% as.vector()
table.pclass = rbind(train.subset, test.subset); colnames(table.pclass) = levels(train$Pclass)
writeLines("\nProportion of passengers stratified by Passenger Class")
table.pclass

train.subset = table(train$Sex) %>% prop.table() %>% round(4) %>% as.vector()
test.subset = table(test$Sex) %>% prop.table() %>% round(4) %>% as.vector()
table.sex = rbind(train.subset, test.subset); colnames(table.sex) = levels(train$Sex)
writeLines("\nProportion of passengers stratified by Passenger Sex")
table.sex


train.subset = table(train$Embarked) %>% prop.table() %>% round(4) %>% as.vector()
test.subset = table(test$Embarked) %>% prop.table() %>% round(4) %>% as.vector()
table.embarked = rbind(train.subset, test.subset); colnames(table.embarked) = levels(train$Embarked) 
writeLines("\nProportion of passengers stratified by Passenger Embarkation Port")
table.embarked

```

1.  There doesn't seem to be any meaningful difference in the distribution of features between the train and test set, except for `Embarked` where the test set has less of proportion of passengers boarding from **S**.
    a.  There are some passengers in train set that are reporting `<blank>` under `Embarked` but there are none like these in the test set.
2.  We now get for the train set features, distribution of their respective values for the Survivors and non-Survivors.
3.  We tabulate the features in the train set that report `NA`s.

```{r eda-2}
plot.age = train %>% ggplot(aes(Age, color = Survived))+ geom_density(size = 1, na.rm = TRUE)+ ylab("")
plot.sibsp = train %>% ggplot(aes(SibSp, color = Survived))+ geom_density(size = 1, na.rm = TRUE)+ ylab("")
plot.parch = train %>% ggplot(aes(Parch, color = Survived))+ geom_density(size = 1, na.rm = TRUE)+ ylab("")
plot.fare = train %>% ggplot(aes(Fare, color = Survived))+ geom_density(size = 1, na.rm = TRUE)+ ylab("")
grid.arrange(plot.age, plot.sibsp, plot.parch, plot.fare)

writeLines("\nSurvival Rate as a function of factor variables\n")
table(Survived = train$Survived, Pclass = train$Pclass) %>% prop.table(2) %>% round(digits = 4)
table(Survived = train$Survived, Sex = train$Sex) %>% prop.table(2) %>% round(digits = 4)
table(Survived = train$Survived, Embarked = train$Embarked) %>% prop.table(2) %>% round(digits = 4)

writeLines("\n")
writeLines("The table below enumerates the features from the train set that report NA values:\n")
na.count(train)

```

We note, visually, that:
1.  There doesn't seem to be a correlation between `Survived` and `Age`, or `SibSp`, or `Parch` or `Fare`.
2.  There is a definite correlation between `Survived` and `Pclass`, and `Sex`, and `Embarked`.
3.  There are some examples in train dataset that have a `<blank>` under the `Embarked` feature and may need imputation.
4.  Finally, the `Age` feature reports 177 `NA`

5.  We get some insight into `Age` by looking at its distribution along `Pclass` and `Sex`.
6.  We also identify the passenger that are reporting `<blank>` under `Embarked`.
    
```{r eda-3}
train %>% ggplot(aes(Age))+ geom_density(na.rm = TRUE)+ facet_grid(Pclass~Sex)

writeLines("Who is reporting NA under Age?\n")
table(Missing_Age = is.na(train$Age), Passenger_Class = train$Pclass, Sex = train$Sex) %>% 
    prop.table(margin = 2) %>% round(digits = 2)

writeLines("Who are the passengers reporing <blank> under Embarked?\n")
train[train$Embarked == "", ]

``` 

1.  The age distributions are approximately normal in all the six classes; however their means differ - decreasing with increasing passenger class number and men are older than women. Also, the `NA` under `Age` come from all the six classes above.
    a.  To impute missing values for `Age`, we get the median values for the six classes above
2.  Two passengers are reporing `<blank>` under `Embarked`; both are travelling together but without any other siblings or parents on board.
    b.  To identify where these two passengers embarked, We get the median fares paid by passengers in first class from the 3 embarkation ports to compare with the fare that these two passengers paid.

```{r eda-4}

age.medians = train %>% group_by(Pclass, Sex) %>% summarise(med = median(Age, na.rm = TRUE))

train %>% filter(Pclass == 1) %>% group_by(Embarked) %>% summarise(median(Fare))

```

### Impute Missing Data

1.  Comparing the fares they paid with median fares for other first class passengers, the 2 passengers with `<blank>` for `Embarked` most likely boarded from **C**. We therefore impute this value to `Embarked` in these two cases.
2.  We impute the median age of the class (function of `Pclass` and `Sex`) to `NA` under `Age`

```{r impute-data-1}

train[train$Embarked == "", "Embarked"] = "C"
train$Embarked = droplevels(train$Embarked)     #   Dropping the unused level
test$Embarked = droplevels(test$Embarked)       #   Repeating the step above for test subset

train[is.na(train$Age) & train$Pclass == 1 & train$Sex == "male", "Age"] = 
    age.medians[age.medians$Pclass == 1 & age.medians$Sex == "male", "med"]
train[is.na(train$Age) & train$Pclass == 2 & train$Sex == "male", "Age"] = 
    age.medians[age.medians$Pclass == 2 & age.medians$Sex == "male", "med"]
train[is.na(train$Age) & train$Pclass == 3 & train$Sex == "male", "Age"] = 
    age.medians[age.medians$Pclass == 3 & age.medians$Sex == "male", "med"]
train[is.na(train$Age) & train$Pclass == 1 & train$Sex == "female", "Age"] = 
    age.medians[age.medians$Pclass == 1 & age.medians$Sex == "female", "med"]
train[is.na(train$Age) & train$Pclass == 2 & train$Sex == "female", "Age"] = 
    age.medians[age.medians$Pclass == 2 & age.medians$Sex == "female", "med"]
train[is.na(train$Age) & train$Pclass == 3 & train$Sex == "female", "Age"] = 
    age.medians[age.medians$Pclass == 3 & age.medians$Sex == "female", "med"]

na.count(train)

```

We have now cleaned the train dataset.

1.  We make a reasonable assumption that the survival of a passenger is not dependent on his/her `Name`, `Ticket` and `Cabin`. For easier subsequent handling, we drop the non-essential features from the train set. Finally, we save a baseline version of the updated train set for future reference.
2.  We make the similar changes (of dropping the non-essential features) from the test set and tabulate the missing data in the set.

```{r pre-proc-2}
feature.set = c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")
train = train[, feature.set]
saveRDS(train, "./RDA/train.Rda")

test = test[, feature.set]

writeLines("The table below enumerates the features from the test dataset that report NA values:\n")
na.count(test)

```

1.  In the test set, the `Age` feature reports 86 `NA`s. To these, we impute the median age of the relevant class (function of `Pclass` and `Sex`) from the **train** subset.
2.  We also identify the passenger record reporting `NA` inder `Fare`

```{r pre-proc-3}
test[is.na(test$Age) & test$Pclass == 1 & test$Sex == "male", "Age"] = 
    age.medians[age.medians$Pclass == 1 & age.medians$Sex == "male", "med"]
test[is.na(test$Age) & test$Pclass == 2 & test$Sex == "male", "Age"] = 
    age.medians[age.medians$Pclass == 2 & age.medians$Sex == "male", "med"]
test[is.na(test$Age) & test$Pclass == 3 & test$Sex == "male", "Age"] = 
    age.medians[age.medians$Pclass == 3 & age.medians$Sex == "male", "med"]
test[is.na(test$Age) & test$Pclass == 1 & test$Sex == "female", "Age"] = 
    age.medians[age.medians$Pclass == 1 & age.medians$Sex == "female", "med"]
test[is.na(test$Age) & test$Pclass == 2 & test$Sex == "female", "Age"] = 
    age.medians[age.medians$Pclass == 2 & age.medians$Sex == "female", "med"]
test[is.na(test$Age) & test$Pclass == 3 & test$Sex == "female", "Age"] = 
    age.medians[age.medians$Pclass == 3 & age.medians$Sex == "female", "med"]

test[is.na(test$Fare),]

```

1.  The passenger record reporting `NA` under `Fare` also reports `Pclass` = 3 and `Embarked` = S. To this `NA`, we impute the median value of the respective class (function of `Pclass` and `Embarked`) from the train subset.
2.  We confirm that the train subset is now clean and save a baseline for future reference.

```{r pre-proc-4}
test[is.na(test$Fare), "Fare"] = train %>% filter(Pclass == 3 & Embarked == "S") %>% 
                                        summarise(med = median(Fare))
na.count(test)

saveRDS(test, "./RDA/test.Rda")
```

### Learning Curves

Before proceeding further, it would be worthwhile to draw some Learning Curves to estimate if a basic logistical regression model suffers from high bias or high variance. This will inform our feature engineering better.

```{r learning-curve}
#   We divide the train set into a train subset and a validation subset in the ratio of 70:30
set.seed(1970)
indx = sample(nrow(train), 0.7*nrow(train), replace = FALSE)
lc.steps = 25; lc.step.size = length(indx)/lc.steps
lc.set.size = seq(lc.step.size, length(indx), length.out = lc.steps) %>% round(digits = 0)

err.train = rep(0, lc.steps); err.val = err.train
for (i in 1:lc.steps) {
  glm.fit = glm(Survived~., data = train[indx[1:lc.set.size[i]],], 
                family = "binomial")
  prob = predict(glm.fit, newdata = train[indx[1:lc.set.size[i]],], type = "response")
  y = train$Survived[indx[1:lc.set.size[i]]]
  y = levels(y)[y] %>% as.numeric()
  err.train[i] = sum(-y*log(prob) - (1-y)*log(1-prob))/(length(y))

  prob = predict(glm.fit, newdata = train[-indx,], type = "response")
  y = train$Survived[-indx]
  y = levels(y)[y] %>% as.numeric()
  err.val[i] = sum(-y*log(prob) - (1-y)*log(1-prob))/(length(y))
}

err.df = cbind(Size = lc.set.size, Train = err.train, Val = err.val) %>% as.data.frame()

err.df %>% ggplot(aes(x = Size)) + geom_line(aes(y = Train, color = "Train.Set.Error"))+ 
    geom_line(aes(y = Val, color = "CrossVal.Set.Error"))+ 
    labs(title = "Learning Curves", x = "Training Set Sample Size", y = "Maximum Likelyhood Error Function")+
    scale_colour_manual(name="Key", values=c(Train.Set.Error ="red", CrossVal.Set.Error ="blue"))

```

1.  The rapid and almost converged learning curves for the train and cross-validation subsets indicate that an un-penalised logistic regression model will suffers from high bias and if we are to improve the performance of our model then we will need to build a model that uses additional features (feature interaction terms, feature power terms, etc).

2.  To help evaluate the relative performance of differnt models built, we will establish a baseline performance measure for test set prediction accuracy using an un-penalised logistic regression model.

```{r est-test-error}
m = nrow(train)
est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.75*m)
    fit = glm(Survived~., data = train[indx,], family = "binomial")
    prob = predict(glm.fit, newdata = train[indx,], type = "response")
    pred = ROCR::prediction(prob, train$Survived[indx])
    perf = performance(pred, measure = "acc")
    max.acc.indx = perf@y.values %>% unlist() %>% as.vector() %>% which.max()
    bestcutoff = perf@x.values %>% unlist %>% as.vector() %>% .[max.acc.indx]
    
    prob = predict(glm.fit, newdata = train[-indx,], type = "response")
    pred = ifelse(prob < bestcutoff, 0, 1)
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using an unpenalised logistic regression is", 
            round(est.test.err/5, digits = 4), sep = " "))

```

1.  We will build different types of prediction models:

    A.  Parametric Models
        i.  Un-penalised logistic regression with feature engineering
        ii.  SVM with different kernels
        iii.  Neural Networks
    B.  Non-parametric models
        i.  Tree based ensembles

```{r cleanup-1}
#   Cleanup to release memory and reduce variable clutter
rm(list = ls(pattern = "plot")); rm(list = ls(pattern = "table")); rm(list = ls(pattern = "lc"))
rm(age.medians, err.df, perf, err.train, err.val, feature.set, test.subset, train.subset)
gc()
```

##  Parametric Models

###  Un-penalised Logistic Regression with hand-crafted Feature Engineering

1.  We employ the *train-test* strategy on the train set to choose the optimal parameters for the different models and estimate test error for a learning algorithm based on such a model.
2.  We will make multiple passes to fit a logistic regression model, each pass learning from the previous one
3.  At each pass we will use the p-values of the included features to decide on additional features to be included or engineered.

#### Logistic Regression: Pass 1

1.  We begin with an unpenalised logistic regression that includes all features.
2.  We divide the train set 60:40 into train and test subsets.
3.  We do a 5-cycle validation to estimate the likely test error.

```{r logit-1}
f1 = paste("Survived", "~", ".", sep = "")
F = as.formula(f1)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx])
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 2

1. The p-values for features indicate that `Parch` and `Fare` are not significant to the model. However we do not drop these features at this point.
2.  We now add engineered features. From the EDA we remember that `Survived` was co-related to `Pclass`, `Sex` and `Embarked`. We add interaction terms including these features to our model.

```{r logit-2}
f2 = paste(f1, "+Pclass:Sex+Pclass:Embarked+Sex:Embarked", sep = "")
F = as.formula(f2)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)
```

#### Logistic Regression: Pass 3

1.  The estimated test error has decreased.
2.  From the summary we infer:
    i.  the interaction term `Pclass:Sex` is significant but `Pclass:Embarked` isn't. We make the necessary changes and rerun the model.
    ii. the term `Embarked` is not significant. However, since we are retaining the interaction term `Sex:Embarked`, we retain the `Embarked` term as well. We also remember from the EDA that the test subset had a noticable difference for `Embarked` from the train subset and that is another reason to retain the term.

```{r logit-3}
f3 = paste(f1, "+Pclass:Sex+Sex:Embarked", sep = "")
F = as.formula(f3)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 4

1.  The estimated test error has reduced.
2.  We add a third order interaction terms for the three variables `Pclass`, `Sex`, `Embarked`.
3.  We also add interaction terms for `Pclass` and `Age` & `Sex` and `Age` as these features are stratified differently.

```{r logit-fit-4}
f4 = paste(f3, "+Pclass:Sex:Embarked+Pclass:Age+Sex:Age", sep = "")
F = as.formula(f4)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)
```

#### Logistic Regression: Pass 5

1.  The estimated test error has increased marginally.
2.  Of the interaction terms added in the last pass, the third order term has no significance to the quality of fit but the other two do. We drop the third order term and refit the model

```{r logit-fit-5}
f5 = paste(f3, "+Pclass:Age+Sex:Age", sep = "")
F = as.formula(f5)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)
```

#### Logistic Regression: Pass 6

1.  The estimated test error remained same.
2.  The interaction term `Sex:Embarked` doesn't seem to be significant to the model. So we drop it.

```{r logit-fit-6}
f6 = paste(f5, "-Sex:Embarked", sep = "")
F = as.formula(f6)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 7

1.  The estimated test error has decreased slightly but all the features in the model except `Fare` and `Parch` are significant to the model. Moreover, `Fare` is a function of `Pclass` and `Embarked`, two terms that are already part of the model. So we will drop `Fare` from the model. 
2.  Since `Pclass`,`Sex` and `Age` are important predictor of survival, we add interaction terms for `Parch` with these 3 features to investigate the impact on the model quality.

```{r logit-fit-7}
f7 = paste(f6, "-Fare+Pclass:Parch+Sex:Parch+Age:Parch", sep = "")
F = as.formula(f7)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 8

1.  The estimated test error is reduced to its minimum so far.
2.  In the latest model the interaction terms `Pclass:Age` and `Sex:Age` are not signifiant to the model. We drop these interaction terms to investigate the impact on the quality.

```{r logit-fit-8}
f8 = paste(f7, "-Pclass:Age-Sex:Age", sep = "")
F = as.formula(f8)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 9

1.  The estimated test error remains same.
2.  We now add interaction terms for `SibSp` with `Pclass`, `Age` and `Sex` to investigate the impact on the quality of the model.
3.  We also add power terms for the two quantitative features `SibSp` and `Age`.

```{r logit-fit-9}
f9 = paste(f8, "+SibSp:Pclass+SibSp:Age+SibSp:Sex+I(SibSp^2)+I(Age^2)", sep = "")
F = as.formula(f9)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 10

1.  The estimated test error has increased marginally.
2.  The three interaction terms added are not significant to the model.
3.  Of the power terms only the `Age^2` term is significant to the model.
4.  We drop the terms from the previous pass that are not relevant and add multiple power terms for `Age` to investigate impact on the quality of the model.

```{r logit-fit-10}
f10 = paste(f8, "+poly(Age, 5)", sep = "")
F = as.formula(f10)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 11

1.  The estimated test error has slightly improved.
2.  More importantly only the 4th order power term for `Age` seems significant to the model.
3.  We make the necessary changes and refit the model.

```{r logit-fit-11}
f11 = paste(f8, "+I(Age^4)", sep = "")
F = as.formula(f11)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

```

#### Logistic Regression: Pass 12

1.  The estimated test error is unchanged. More importantly, all terms except `Age:Parch` in the model are significant to the quality of the model.
2.  We drop the non-significant term and rebuild the model.

```{r logit-fit-12}
f12 = paste(f11, "-Age:Parch", sep = "")
F = as.formula(f12)

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    fit = glm(F, data = train[indx,], family = "binomial") 
    prob = predict(fit, newdata = train[indx,], type = "response") 
    pred = ROCR::prediction(prob, train$Survived[indx]) 
    perf = performance(pred, measure = "acc") 
    bestaccindx = perf@y.values %>% unlist() %>% which.max() 
    bestcutoff = perf@x.values %>% unlist() %>% .[bestaccindx] 
    
    prob = predict(fit, newdata = train[-indx,], type = "response") 
    pred = ifelse(prob < bestcutoff, 0, 1) 
    test.err = mean(pred != train$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using this logistic fit is", 
            round(est.test.err/5, digits = 4), sep = " "))

fit = glm(F, data = train, family = "binomial")

summary(fit)

               
```

1.  We now have a model with the lowest estimated test error and with all terms significant to the model quality.
2.  We save this model for future reference.

```{r save-model}
saveRDS(fit, "./RDA/best.logit.Rda")

```


```{r cleanup-2}
#   Cleanup to release memory and reduce variable clutter
rm(list = ls(pattern = "f")); rm(F)
gc()
```

### Support Vector Machine with Various Kernels

1.  We use the *train-validate-test* strategy to choose the optimal parameters for the different models and then estimate test error for a learning algorithm based on such a model.
2.  We implement a SVM with different kernels - linear, radial and polynomial.

####    SVM with Linear Kernel

```{r svm-linear}
cost = c(0.01, 0.1, 1, 10, 100, 1000)

est.test.err = 0; 
for (k in 1:5) {
  set.seed(k)
  indx = split(sample(1:m, m), f = c(rep("train", 6), rep("cval", 2), rep("test", 2)))
  
  #     Fitting an svm to train subset and using using the cval subset to determine the optimal cost for the SVM model using accuracy as the measure
  bestcost = 1; leasterr = 1;
  for (c in cost) {
        fit = svm(Survived~., data = train[indx$train,], kernel = "linear", cost = c, 
                  scale = TRUE, fitted = FALSE)
        pred = predict(fit, newdata = train[indx$cval,])
        err = mean(pred != train$Survived[indx$cval])
        if (err < leasterr) {
            leasterr = err
            bestcost = c 
        }
    }
  fit = svm(Survived~., data = train[indx$train,], kernel = "linear", cost = bestcost, 
            scale = TRUE, fitted = FALSE)
  pred = predict(fit, newdata = train[indx$test,])
  test.err = mean(pred != train$Survived[indx$test])
  est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using an SVM with a linear kernel is", 
            round(est.test.err/5, digits = 4), sep = " "))

```

####    SVM with Radial Kernel

```{r svm-radial}
cost = c(0.01, 0.1, 1, 10, 100, 1000)
gamma = c(0.25, 0.125, 0.5, 1, 2)

est.test.err = 0; 
for (k in 1:5) {
  set.seed(k)
  indx = split(sample(1:m, m), f = c(rep("train", 6), rep("cval", 2), rep("test", 2)))
  
  # Fitting an svm to train subset and using using the cval subset to determine the optimal cost and gamma for the SVM model using accuracy as the measure
  bestcost = 1; bestgamma = 0.05; leasterr = 1;
  for (c in cost) {
    for (g in gamma) {
        fit = svm(Survived~., data = train[indx$train,], kernel = "radial", cost = c, gamma = g,
                      scale = TRUE, fitted = FALSE)
        pred = predict(fit, newdata = train[indx$cval,])
        err = mean(pred != train$Survived[indx$cval])
        if (err < leasterr) {
            leasterr = err
            bestcost = c 
            bestgamma = g
      }
    }
  }
  fit = svm(Survived~., data = train[indx$train,], kernel = "radial", cost = bestcost, gamma = bestgamma, 
                scale = TRUE, fitted = FALSE)
  pred = predict(fit, newdata = train[indx$test,])
  test.err = mean(pred != train$Survived[indx$test])
  est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using an SVM with a radial kernel is", 
            round(est.test.err/5, digits = 4), sep = " "))

```

####    SVM with Polynomial Kernel

```{r svm-polynomial}
cost = c(0.01, 0.1, 1, 10, 100)
degree = 2:4
coeff0 = seq(0, 1, length.out = 5)
gamma = c(0.025, 0.05, 0.125, 0.25, 0.5)

est.test.err = 0; 
for (k in 1:5) {
    set.seed(k)
    indx = split(sample(1:m, m), f = c(rep("train", 6), rep("cval", 2), rep("test", 2)))
    # Fitting an svm to train subset and using using the cval subset to determine the optimal degree, cost and gamma for the SVM model using accuracy as the measure
    bestcost = 1; bestdegree = 3; bestcoeff0 = 0; leasterr = 1; bestgamma = 0.125
    for (c in cost) {
        for (d in degree) {
            for (a in coeff0) {
                for (g in gamma) {
                    fit = svm(
                        Survived ~ ., data = train[indx$train,], kernel = "polynomial",
                        cost = c, degree = d, coef0 = a, gamma = g,
                        scale = TRUE, fitted = FALSE)
                    pred = predict(fit, newdata = train[indx$cval,])
                    err = mean(pred != train$Survived[indx$cval])
                    if (err < leasterr) {
                        leasterr = err; bestcost = c
                        bestdegree = d; bestcoeff0 = a; bestgamma = g
                    }
                }
            }
        }
    }
    fit = svm(Survived ~ ., data = train[indx$train,], kernel = "polynomial",
        cost = bestcost, degree = bestdegree, coef0 = bestcoeff0, gamma = bestgamma,
        scale = TRUE, fitted = FALSE)
    pred = predict(fit, newdata = train[indx$test,])
    test.err = mean(pred != train$Survived[indx$test])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using an SVM with a polynomial kernel is", 
            round(est.test.err/5, digits = 4), sep = " "))

```

Overall, the SVM as a learning algorithm did not meaningfully reduce the estimated test error with the SVM with a *polynomial* kernel delivering the least estimated test error.

### Neural Networks

We implement a neural network to gauge the algorighm's effectiveness in addressing the problem. Specifically we build four different neural networks:
1.  A neural network with 1 hidden layer and as many neurons as features.
2.  A neural network with 2 hidden layers and in each layer as many neurons as features.
3.  A neural network with 1 hidden layer and 1.5 times as many neurons as features.
4.  A neural network with 2 hidden layers and in each layer 1.5 times as many neurons as features.

Comparing the results of these 4 neural network models will hopefully unearth the direction in which we need to optimise the structure of the neural network. 

####    Neural Network - 1 hidden layer, 10 neurons

```{r nnet-1}
p = ncol(train)-1

est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    x = model.matrix(Survived~., data = train)[,-1]     #   Convert to a model matrix neuralnet() will only                                                                 accept numerical inputs for features
    
    
    #   We normalise the data but only using the train subset examples. This is to avoid test subset from participating in training the model.
    mu = scale(x[indx,]) %>% attr("scaled:center")
    sdev = scale(x[indx,]) %>% attr("scaled:scale")
    x = scale(x, center = mu, scale = sdev) %>% as.data.frame()
    x$Survived = train$Survived
    
    set.seed(k)
    fit = neuralnet(Survived~., data = x[indx,], hidden = p, threshold = 0.02, linear.output = FALSE)
    
    pred = predict(fit, newdata = x[-indx,], all.units = FALSE)
    pred = apply(pred, 1, which.max)
    pred = factor(pred, labels = c(0,1))
    test.err = mean(pred != x$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error for a neural network with 1 hidden layer and 10 neurons in the hidden layer is", round(est.test.err/5, digits = 4), sep = " "))
```

1.  The estimated test error for a neural network with 1 hidden layer and 10 neurons in the hidden layer forms the baseline for the performance measure.

####    Neural Network - 2 hidden layers, 10 neurons per hidden layer

```{r nnet-2}
est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    x = model.matrix(Survived~., data = train)[,-1]

    mu = scale(x[indx,]) %>% attr("scaled:center")
    sdev = scale(x[indx,]) %>% attr("scaled:scale")
    x = scale(x, center = mu, scale = sdev) %>% as.data.frame()
    x$Survived = train$Survived
    p = ncol(x)-1
    
    set.seed(k)
    fit = neuralnet(Survived~., data = x[indx,], hidden = c(p,p), threshold = 0.12, linear.output = FALSE)
    
    pred = predict(fit, newdata = x[-indx,], all.units = FALSE)
    pred = apply(pred, 1, which.max)
    pred = factor(pred, labels = c(0,1))
    test.err = mean(pred != x$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error for a neural network with 2 hidden layers and 10 neurons per hidden layer is", round(est.test.err/5, digits = 4), sep = " "))

```

1.  There is a reasonable increase in the estimated test error over the baseline.

####    Neural Network - 1 hidden layer, 15 neurons

```{r nnet-3}
est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    x = model.matrix(Survived~., data = train)[,-1]
    mu = scale(x[indx,]) %>% attr("scaled:center")
    sdev = scale(x[indx,]) %>% attr("scaled:scale")
    x = scale(x, center = mu, scale = sdev) %>% as.data.frame()
    x$Survived = train$Survived
    
    set.seed(k)
    fit = neuralnet(Survived~., data = x[indx,], hidden = 1.5*p, threshold = 0.03, linear.output = FALSE)
    
    pred = predict(fit, newdata = x[-indx,], all.units = FALSE)
    pred = apply(pred, 1, which.max)
    pred = factor(pred, labels = c(0,1))
    test.err = mean(pred != x$Survived[-indx])
    est.test.err = test.err + est.test.err
}
print(paste("The estimated test error for a neural network with 1 hidden layer and 15 neurons per hidden layer is", round(est.test.err/5, digits = 4), sep = " "))

```

1.  There is a reasonable increase in the estimated test error over the baseline.

####    Neural Network - 2 hidden layers, 15 neurons per hidden layer

```{r nnet-4}
est.test.err = 0
for (k in 1:5) {
    set.seed(k)
    indx = sample(1:m, 0.6*m)
    x = model.matrix(Survived~., data = train)[,-1]
    mu = scale(x[indx,]) %>% attr("scaled:center")
    sdev = scale(x[indx,]) %>% attr("scaled:scale")
    x = scale(x, center = mu, scale = sdev) %>% as.data.frame()
    x$Survived = train$Survived
    
    set.seed(k)
    fit = neuralnet(Survived~., data = x[indx,], hidden = c(1.5*p, 1.5*p), threshold = 0.05)
    
    pred = predict(fit, newdata = x[-indx,], all.units = FALSE)
    pred = apply(pred, 1, which.max)
    pred = factor(pred, labels = c(0,1))
    test.err = mean(pred != x$Survived[-indx])
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error for a neural network with 2 hidden layer and 15 neurons per hidden layer is", round(est.test.err/5, digits = 4), sep = " "))
```

1.  The estimated test error increased significantly over the baseline value.
2.  Overall, the neural network learning model doesn't seem appropriate for this particular problem.

```{r cleanup-3}
#   Cleanup to release memory and reduce variable clutter
rm(x)
gc()
```

##  Non-parametric models

###  Random Forest

1.  Since the *random forest* algorithm uses a *bootstrap* sampling strategy, the function `randomforest()` offers an easy built-in way to estimate test error without the need for an explicit *train-test* strategy.

```{r randomforest}
est.test.err = 0; 
trees.n = c(100, 500, 2500, 125000)
for (t in trees.n) {
    fit = randomForest(Survived ~ ., data = train)
    test.err = 1-(fit$confusion[1,1]+fit$confusion[2,2])/m
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using a random forest ensemble is", 
            round(est.test.err/4, digits = 4), sep = " "))

```

## Bagged Trees

1. Similar strategy to the *random forest* algorithm above

```{r bagging}
est.test.err = 0; 
trees.n = c(100, 500, 2500, 12500)
for (t in trees.n) {
    fit = randomForest(Survived ~ ., data = train, mtry = p)
    test.err = 1-(fit$confusion[1,1]+fit$confusion[2,2])/m
    est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using a bagged forest ensemble is", 
            round(est.test.err/4, digits = 4), sep = " "))
```

###  Boosted Trees

1.  The `gbm()` function requires that the dependent/response variable be numerical. For the train dataset we need to convert `Survived` from `<fct>` to `<int>`.
2.  For the `gbm()` fit, we need to optimise 3 parameters - number of trees (`n.trees`), depth of each tree (`interaction.depth`) and learning rate (`shrinkage`).
3.  The optimal value for `n.trees`, in turn, depends on `interaction.depth` and `shrinkage` and for a fit, i.e. specific values of `interaction.depth` and `shrinkage`, can be obtained by setting the parameter `cv.folds > 1`, and then recovering the best value through `gbm.perf()` function.
4.  For the other two parameters we use grid search, setting *accuracy* as the measure to optimise to.
5. The `predict.gbm()` function returns the *logit* values for examples and need to be converted to *Probability(Y = 1|x)* by the appropriate transformation

```{r boosted-trees}
train$Survived = as.numeric(levels(train$Survived))[train$Survived]

depth = 1:4
alpha = c(0.1, 0.01, 0.001)

est.test.err = 0;
for (k in 1:5) {
  set.seed(k)
  indx = split(sample(1:m, m), f = c(rep("train", 6), rep("cval", 2), rep("test", 2)))
  
  bestd = 1; bestalpha = 1; bestcutoff = 0; besttrees.n = 1000; leasterr = 1
  for (d in depth) {
    for (a in alpha) {
        fit = gbm(Survived~., data = train[indx$train,], distribution = "bernoulli", cv.folds = 5,
                        n.trees = 5000, interaction.depth = d, shrinkage = a, 
                        class.stratify.cv = TRUE, keep.data = FALSE)
        trees.n = gbm.perf(fit, plot.it = FALSE, method = "cv")
        
        pred = predict(fit, newdata = train[indx$train,], n.trees = trees.n)
        prob = exp(pred)/(1+exp(pred))
        
        pred = ROCR::prediction(prob, factor(train$Survived[indx$train]))   #   Using the ROCR library
        perf = performance(pred, measure = "acc")                           #   to decide the best threshold                                                                               #   probability to classify an                                                                                 #   example as '1'
        max.acc.indx = perf@y.values %>% unlist() %>% which.max()   
        cutoff = perf@x.values %>% unlist() %>% .[max.acc.indx]
        
        pred = predict(fit, newdata = train[indx$cval,], n.trees = trees.n)
        prob = exp(pred)/(1+exp(pred))
        pred = ifelse(prob < cutoff, 0, 1)
        err = mean(pred != train$Survived[indx$cval])
        if (err < leasterr) {
            leasterr = err
            bestd = d 
            bestalpha = a
            bestcutoff = cutoff
            besttrees.n = trees.n
      }
    }
  }
  
  fit = gbm(Survived~., data = train[indx$train,], distribution = "bernoulli", n.trees = besttrees.n, 
                  interaction.depth = bestd, shrinkage = bestalpha, keep.data = FALSE)
  
  pred = predict(fit, newdata = train[indx$test,], n.trees = besttrees.n)
  prob = exp(pred)/(1+exp(pred))
        
  pred = ifelse(prob < bestcutoff, 0, 1)
  
  test.err = mean(pred != train$Survived[indx$test])
  est.test.err = test.err + est.test.err
}

print(paste("The estimated test error using a boosted tree model is", 
            round(est.test.err/5, digits = 4), sep = " "))

train$Survived = factor(train$Survived)
```

##  Predictions

1.  Comparing the multiple learning algorithms employed using the estimated test error as a measure of success, we shortlist the following 3 to predict the dependent variable for the test set.
    a.  Unpenalised Logistic Regression with hand-crafted feature engineering (est. test set error = 0.1681)
    b.  Random Forest (est. test set error = 0.1684)
    c.  Bagged Tree ensemble (est. test set error = 0.193)
    d.  Boosted Tree ensemble (estimated test set error = 0.2)

###  Random Forest

1.  We use the *train-validate* strategy using the train dataset to decide on the optimal number of trees to grow for the ensemble.
2.  We build a final ensemble using the entire train dataset and for optimal number of trees decided by the earlier step.

```{r predict-randomforest}
m = nrow(train)
dat = rbind.data.frame(train, test)

trees.n = c(100, 500, 2500, 12500)
leasterr = 1; bestn = 500
for (t in trees.n) {
    fit = randomForest(Survived~., data = train, ntree = t, keep.forest = FALSE)
    err = 1-(fit$confusion[1,1]+fit$confusion[2,2])/m
    if (err < leasterr) {
        leasterr = err
        bestn = t
    }
}

#   Important to keep.forest = TRUE so predictions can be made
fit = randomForest(Survived~., data = train, ntree = bestn, keep.forest = TRUE) 
pred.rf = predict(fit, newdata = test, type = "response")
pred.rf = as.numeric(levels(pred.rf))[pred.rf]

out = cbind(PassengerId = rownames(test), Survived = pred.rf)
write.csv(out, "./data/out.rf.csv", quote = FALSE, row.names = FALSE)

```

###  Bagged Tree Ensemble

1.  We use the *train-validate* strategy using the train dataset to decide on the optimal number of trees to grow for the ensemble.
2.  We build a final ensemble using the entire train dataset and for optimal number of trees decided by the earlier step.

```{r predict-baggedtree}
p = ncol(train)-1

trees.n = c(100, 500, 2500, 12500)
leasterr = 1; bestn = 500
for (t in trees.n) {
    fit = randomForest(Survived~., data = train, ntree = t, mtry = p, keep.forest = FALSE)
    err = 1-(fit$confusion[1,1]+fit$confusion[2,2])/m
    if (err < leasterr) {
        leasterr = err
        bestn = t
    }
}

#   Important to keep.forest = TRUE so predictions can be made
fit = randomForest(Survived~., data = train, ntree = bestn, mtry = p, keep.forest = TRUE) 
pred.bag = predict(fit, newdata = test, type = "response")
pred.bag = as.numeric(levels(pred.bag))[pred.bag]

out = cbind(PassengerId = rownames(test), Survived = pred.bag)
write.csv(out, "./data/out.bag.csv", quote = FALSE, row.names = FALSE)
```

###  Boosted Tree Ensemble

1.  The `gbm()` function requires that the dependent/response variable be numerical. For the train dataset we need to convert `Survived` from `<fct>` to `<int>`.
2.  We use the *train-validate* strategy using the train dataset to optimise 3 parameters - number of trees (`n.trees`), depth of each tree (`interaction.depth`) and learning rate (`shrinkage`).
3.  The optimal value for `n.trees`, in turn, depends on `interaction.depth` and `shrinkage` and for a fit, i.e. specific values of `interaction.depth` and `shrinkage`, can be obtained by setting the parameter `cv.folds > 1`, and then recovering the best value through `gbm.perf()` function.
4.  For the other two parameters we use grid search, setting *accuracy* as the measure to optimise to.
5. The `predict.gbm()` function returns the *logit* values for examples and need to be converted to *Probability(Y = 1|x)* by the appropriate transformation

```{r predict-boostedtrees}
train$Survived = as.numeric(levels(train$Survived))[train$Survived]

depth = 1:4
alpha = c(0.1, 0.01, 0.001)

est.test.err = 0;
for (k in 1:5) {
  set.seed(k)
  indx = sample(1:m, 0.8*m)
  
  bestd = 1; bestalpha = 1; bestcutoff = 0; besttrees.n = 1000; leasterr = 1
  for (d in depth) {
    for (a in alpha) {
        fit = gbm(Survived~., data = train[indx,], distribution = "bernoulli", cv.folds = 5,
                        n.trees = 5000, interaction.depth = d, shrinkage = a, 
                        class.stratify.cv = TRUE, keep.data = FALSE)
        trees.n = gbm.perf(fit, plot.it = FALSE, method = "cv")
        
        pred = predict(fit, newdata = train[indx,], n.trees = trees.n)
        prob = exp(pred)/(1+exp(pred))
        
        pred = ROCR::prediction(prob, train$Survived[indx])     #   Using the ROCR library
        perf = performance(pred, measure = "acc")               #   to decide the best threshold                                                                               #   probability to classify an example as '1'
        max.acc.indx = perf@y.values %>% unlist() %>% which.max()     
        cutoff = perf@x.values %>% unlist() %>% .[max.acc.indx]
        
        pred = predict(fit, newdata = train[-indx,], n.trees = trees.n)
        prob = exp(pred)/(1+exp(pred))
        pred = ifelse(prob < cutoff, 0, 1)
        err = mean(pred != train$Survived[-indx])
        if (err < leasterr) {
            leasterr = err
            bestd = d 
            bestalpha = a
            bestcutoff = cutoff
            besttrees.n = trees.n
      }
    }
  }
}  
  
fit = gbm(Survived~., data = train[indx,], distribution = "bernoulli", n.trees = besttrees.n, 
          interaction.depth = bestd, shrinkage = bestalpha, keep.data = FALSE)
  
pred = predict(fit, newdata = test, n.trees = besttrees.n)
prob = exp(pred)/(1+exp(pred))
        
pred.boost = ifelse(prob < bestcutoff, 0, 1)

out = cbind(PassengerId = rownames(test), Survived = pred.boost)
write.csv(out, "./data/out.boost.csv", quote = FALSE, row.names = FALSE)

train$Survived = factor(train$Survived)
```

###  Un-penalised Logistic Regression with hand-crafted Feature Engineering

1.  We use the best model previously iteratively built.

```{r predict-logistic-regression}
fit = readRDS("./RDA/best.logit.Rda")   #   Loading the best logistic model previously built

prob = predict(fit, newdata = train, type = "response")
pred = ROCR::prediction(prob, train$Survived)         
perf = performance(pred, measure = "acc")                   
max.acc.indx = perf@y.values %>% unlist() %>% which.max()   
cutoff = perf@x.values %>% unlist() %>% .[max.acc.indx]

prob = predict(fit, newdata = test, type = "response")
pred.logit = ifelse(prob < cutoff, 0, 1)

out = cbind(PassengerId = rownames(test), Survived = pred.logit)
write.csv(out, "./data/out.logit.feat.engg.csv", quote = FALSE, row.names = FALSE)

```

##  Conclusion

1.  We used four models to make predictions for the test dataset. The results, from the Kaggle evaluation, are as follows:

**Model                               Est. Test Error             Act. Test Error**
Un-penalised logistic regression        
with hand-crafted feature engg.         0.1681                      0.2440

Random Forest ensemble                  0.1684                      0.2249

Bagged Trees ensemble                   0.193                       0.2440

Boosted Trees ensemble                  0.2                         0.2201

2.  Expectedly, for all the 4 models, the actual test error was greater than the estimated test error.
3.  Of the four, the Boosted Trees ensemble model performed the best and more importantly delivered the least difference between the actual and estimated test error.








